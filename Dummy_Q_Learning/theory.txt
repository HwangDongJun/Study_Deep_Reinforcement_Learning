OpenAI_GYM에서 보았던 Frozen Lake게임에서 다음으로 가야할 길을 보지 못할 경우 Q_Learning을 실행한다.
길을 모르니 랜덤하게 가는 것은 매우 비효율적이다.

해결법은 ask. 아는 길도, 물어가라. -> (Q)
(1)state / (2)action에 관한 입력으로 Q에게 준다면 (3)quality(reward)를 준다.
간단하게 이것을 Q-function이라고 한다. => Q(state, action)
ex) Q(sl, LEFT) : 0
    Q(sl, RIGHT) : 0.5
    Q(sl, UP) : 0
    Q(sl, DOWN) : 0.3   여기서 가장 큰 수를 찾는다. 0.5로 RIGHT이다.
수학적 표시로써 max는 Q가 가질 수 있는 최대값이며, Q가 최대값이 되도록 하는 것으로 Q.PNG참고
pi^*표시를 우리가 가장 최적으로 움직일 수 있는 방법이기에 Optimal Policy라고 한다.

어떻게 표시를 할 수 있을까?
Q(s, a) = r + max_a' Q(s', a')
현재 내가 s상태에 있으며, a의 action을 취해서 받을 수 있는 reward인 r과 + max_a' Q(s' ,a') => 4가지의 방향의 action중의 최고값이다. 즉, 다음 상태에서의 4방향중의 최대값을 이야기 한다.(미리 있는 값들중)
reward의 경우도 r_1부터 r_n(해당목적지)까지 받는다고 생각을 해보면 R = r_1 + r_2 + ... + r_n이며, R_t = r_t + r_t+1 + ... + r_n이다.
그러므로 Q와 동일하게 R_t^* => 목적지까지의 이동 경로중 가장 최적인 경우 => r_t + max R_t+1이다.
(Learning_Q.PNG참고)

Dummy Q_Learning algorithm
1. 표를 만들어서 4개의 방향을 만들어 전부 0으로 초기화한다.
2. 현재 상태에서 state와 action으로 Q(s, a) = r + max_a' Q(s', a') 공식을 이용하여 계속해서 지나간다.
3. 목적지에 도착을 했을 경우 1을 반환 받을 수 있으며, 그런 값을 통해 계속적인 진행으로 max인 부분을 만들어간다.