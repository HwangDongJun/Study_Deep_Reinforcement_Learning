Dummy_Q_Learning의 좋지 않은 점을 보완하기 위한 방법

* Exploit VS Exploration    (action부분)
이전의 Dummy는 이전의 만들어진 길만을 이용했지만, 새로운 길을 가면서 새로운 학습을 할 수 있다.
(기존에 존재하는 길이 아닌 새로운 길)

=> E-greedy
e = 0.1
if rand < e:
    a = random -> 10% 랜덤하게 간다.
else:
    a = argmax(Q(s,a)) -> 90% 아는 길로 간다.

=> decaying E-greedy
이 과정을 진행하다보면 학습을 많이 진행을 함으로써, 랜덤하게 가는 횟수를 줄인다.
e = 0.1 / (i+1)     [i는 for로 range만큼 학습을 진행하는 값]

=> add random noise
기존의 존재하는 값에 + random_values를 더하게 된다. 그리고 argmax로 가장 큰 값을 진행한다.
물론 여기서도 많은 진행이 되었다면, / (i+1)로 횟수를 줄인다.

마지막으로 Q(s, a) = r + max_a' Q(s', a') 수식을 좀 더 고쳐야 한다.
max_a' 앞 부분에 γ(감마)를 곱하는 것이다.
Q(s, a) = r + γ * max_a' Q(s', a') 이 값을 만약 0.9로 둔다면, goal지점 바로 앞이 1인 경우 시작하는 지점쪽으로 돌아가면서 값을 계산해보자.
(Table.PNG참고) 사진에서 보면 해당하는 값으로 채워진다면 길을 찾는 입장에서는 더 빠른 길을 찾을 수 있을 것이다.